{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f705d4-00dc-4270-986a-a37f5f4ecc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c479402-b7ba-47f6-b64d-b2afb78a97a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data files: ['olist_sellers_dataset.csv', 'product_category_name_translation.csv', 'olist_orders_dataset.csv', 'olist_order_items_dataset.csv', 'olist_customers_dataset.csv', 'olist_geolocation_dataset.csv', 'olist_order_payments_dataset.csv', 'olist_order_reviews_dataset.csv', 'olist_products_dataset.csv']\n",
      "✅ BigQuery library installed\n",
      "❌ Authentication needed: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Check installations and data\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Check if data files exist\n",
    "data_path = \"/Users/joehaldenby/Portfolio_Projects/Brazil_commerce_analysis/data\"\n",
    "print(\"Data files:\", os.listdir(data_path))\n",
    "\n",
    "# Try importing BigQuery\n",
    "try:\n",
    "    from google.cloud import bigquery\n",
    "    print(\"✅ BigQuery library installed\")\n",
    "except ImportError:\n",
    "    print(\"❌ Need to install: pip install google-cloud-bigquery\")\n",
    "\n",
    "# Check for Google Cloud authentication\n",
    "try:\n",
    "    client = bigquery.Client()\n",
    "    print(\"✅ Google Cloud authenticated\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Authentication needed:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87600896-2625-45a9-b08d-04624870dc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully authenticated with service account!\n",
      "Found 0 existing datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Set the path to your service account key\n",
    "credential_path = \"/Users/joehaldenby/Portfolio_Projects/Brazil_commerce_analysis/credentials/brazil-commerce-analysis-226acdd2c335.json\"\n",
    "\n",
    "# Set environment variable\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n",
    "\n",
    "# Test connection\n",
    "project_id = \"brazil-commerce-analysis\"\n",
    "\n",
    "try:\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    print(\"✅ Successfully authenticated with service account!\")\n",
    "    \n",
    "    # Test by listing datasets\n",
    "    datasets = list(client.list_datasets())\n",
    "    print(f\"Found {len(datasets)} existing datasets\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"❌ Authentication failed:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76b30cce-58ba-48c9-b355-a7cc9c8a2955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created dataset: Brazil_commerce\n",
      "\n",
      "Found 9 CSV files:\n",
      "  - olist_sellers_dataset.csv\n",
      "  - product_category_name_translation.csv\n",
      "  - olist_orders_dataset.csv\n",
      "  - olist_order_items_dataset.csv\n",
      "  - olist_customers_dataset.csv\n",
      "  - olist_geolocation_dataset.csv\n",
      "  - olist_order_payments_dataset.csv\n",
      "  - olist_order_reviews_dataset.csv\n",
      "  - olist_products_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset for your ecommerce data\n",
    "dataset_id = \"Brazil_commerce\"\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "\n",
    "try:\n",
    "    # Try to create the dataset\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = \"US\"  # or \"EU\" if you prefer\n",
    "    dataset = client.create_dataset(dataset)\n",
    "    print(f\"✅ Created dataset: {dataset_id}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(f\"✅ Dataset {dataset_id} already exists\")\n",
    "    else:\n",
    "        print(f\"❌ Error creating dataset: {e}\")\n",
    "\n",
    "# List your CSV files\n",
    "data_path = \"/Users/joehaldenby/Portfolio_Projects/Brazil_commerce_analysis/data\"\n",
    "csv_files = [f for f in os.listdir(data_path) if f.endswith('.csv')]\n",
    "print(f\"\\nFound {len(csv_files)} CSV files:\")\n",
    "for file in csv_files:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25a1f8d7-a5ea-404c-9877-881fea01efe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📤 Uploading olist_orders_dataset.csv -> orders...\n",
      "✅ Uploaded 99,441 rows to orders\n",
      "\n",
      "📤 Uploading olist_customers_dataset.csv -> customers...\n",
      "✅ Uploaded 99,441 rows to customers\n",
      "\n",
      "📤 Uploading olist_sellers_dataset.csv -> sellers...\n",
      "✅ Uploaded 3,095 rows to sellers\n",
      "\n",
      "📤 Uploading olist_products_dataset.csv -> products...\n",
      "✅ Uploaded 32,951 rows to products\n",
      "\n",
      "📤 Uploading olist_order_items_dataset.csv -> order_items...\n",
      "✅ Uploaded 112,650 rows to order_items\n",
      "\n",
      "📤 Uploading olist_order_payments_dataset.csv -> order_payments...\n",
      "✅ Uploaded 103,886 rows to order_payments\n",
      "\n",
      "📤 Uploading olist_order_reviews_dataset.csv -> order_reviews...\n",
      "✅ Uploaded 99,224 rows to order_reviews\n",
      "\n",
      "📤 Uploading olist_geolocation_dataset.csv -> geolocation...\n",
      "✅ Uploaded 1,000,163 rows to geolocation\n",
      "\n",
      "📤 Uploading product_category_name_translation.csv -> category_translation...\n",
      "✅ Uploaded 71 rows to category_translation\n",
      "\n",
      "🎉 All tables uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Define table mappings (CSV filename -> BigQuery table name)\n",
    "table_mappings = {\n",
    "    'olist_orders_dataset.csv': 'orders',\n",
    "    'olist_customers_dataset.csv': 'customers', \n",
    "    'olist_sellers_dataset.csv': 'sellers',\n",
    "    'olist_products_dataset.csv': 'products',\n",
    "    'olist_order_items_dataset.csv': 'order_items',\n",
    "    'olist_order_payments_dataset.csv': 'order_payments',\n",
    "    'olist_order_reviews_dataset.csv': 'order_reviews',\n",
    "    'olist_geolocation_dataset.csv': 'geolocation',\n",
    "    'product_category_name_translation.csv': 'category_translation'\n",
    "}\n",
    "\n",
    "# Upload each file\n",
    "for csv_file, table_name in table_mappings.items():\n",
    "    print(f\"\\n📤 Uploading {csv_file} -> {table_name}...\")\n",
    "    \n",
    "    # Read CSV\n",
    "    df = pd.read_csv(f\"{data_path}/{csv_file}\")\n",
    "    \n",
    "    # Upload to BigQuery\n",
    "    table_id = f\"{client.project}.Brazil_commerce.{table_name}\"\n",
    "    \n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()  # Wait for job to complete\n",
    "    \n",
    "    print(f\"✅ Uploaded {len(df):,} rows to {table_name}\")\n",
    "\n",
    "print(\"\\n🎉 All tables uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fae061d-2e67-447e-bf25-5857e1be7d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client created successfully!\n",
      "\n",
      "📋 Tables in Brazil_commerce dataset:\n",
      "  ✅ category_translation: 71 rows, 2 columns\n",
      "  ✅ customers: 99,441 rows, 5 columns\n",
      "  ✅ geolocation: 1,000,163 rows, 5 columns\n",
      "  ✅ order_items: 112,650 rows, 7 columns\n",
      "  ✅ order_payments: 103,886 rows, 5 columns\n",
      "  ✅ order_reviews: 99,224 rows, 7 columns\n",
      "  ✅ orders: 99,441 rows, 8 columns\n",
      "  ✅ products: 32,951 rows, 9 columns\n",
      "  ✅ sellers: 3,095 rows, 4 columns\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Use the correct credentials file path\n",
    "credential_path = \"/Users/joehaldenby/Portfolio_Projects/Brazil_commerce_analysis/credentials/brazil-commerce-analysis-226acdd2c335.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n",
    "\n",
    "# Create client\n",
    "client = bigquery.Client()\n",
    "print(\"✅ Client created successfully!\")\n",
    "\n",
    "# Check all tables in your dataset\n",
    "tables = client.list_tables(\"Brazil_commerce\")\n",
    "\n",
    "print(\"\\n📋 Tables in Brazil_commerce dataset:\")\n",
    "for table in tables:\n",
    "    # Get table info\n",
    "    table_ref = client.get_table(table)\n",
    "    print(f\"  ✅ {table.table_id}: {table_ref.num_rows:,} rows, {len(table_ref.schema)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b8b13-676f-48a4-96c7-8c22caf14433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
